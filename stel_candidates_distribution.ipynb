{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import kl_div\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "params = {'results_path': 'results',\n",
    "          'data_path': 'data',\n",
    "          'nfp': 2,\n",
    "          'test_size': 0.2,\n",
    "          'random_state': 42,\n",
    "          'n_components': 5,\n",
    "          'n_samples': 1000,\n",
    "          'model': 'nn'\n",
    "          }\n",
    "this_path = str(os.path.abspath(''))\n",
    "general_results_path = os.path.join(this_path, params['results_path'])\n",
    "results_path = os.path.join(general_results_path, f'nfp{params[\"nfp\"]}')\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "model_path = os.path.join(params['results_path'], f\"nfp{params['nfp']}\", f\"nn_qsc_nfp{params['nfp']}_model{params['model']}.h5\")\n",
    "scaler_x_path = os.path.join(params['results_path'], f\"nfp{params['nfp']}\", f\"nn_qsc_nfp{params['nfp']}_scaler_x.pkl\")\n",
    "scaler_y_path = os.path.join(params['results_path'], f\"nfp{params['nfp']}\", f\"nn_qsc_nfp{params['nfp']}_scaler_y.pkl\")\n",
    "model = load_model(model_path)\n",
    "scaler_x = joblib.load(scaler_x_path)\n",
    "scaler_y = joblib.load(scaler_y_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "filename = os.path.join(this_path, params['data_path'], f'qsc_out.random_scan_nfp{params[\"nfp\"]}.parquet')\n",
    "df = pd.read_parquet(filename)\n",
    "\n",
    "# Byte order fix\n",
    "for column in df.columns:\n",
    "    if df[column].dtype.byteorder == '>':\n",
    "        df[column] = df[column].values.byteswap().newbyteorder()\n",
    "\n",
    "# Drop ysum column if exists\n",
    "if 'ysum' in df.columns:\n",
    "    df = df.drop(columns='ysum')\n",
    "\n",
    "# Define columns\n",
    "x_columns = [col for col in df.columns if col.startswith('x')]\n",
    "y_columns = [col for col in df.columns if col.startswith('y')]\n",
    "\n",
    "# Split data\n",
    "Y = df[x_columns].values\n",
    "X = df[y_columns].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=params['test_size'], random_state=params['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM for all data\n",
    "gmm_all = GaussianMixture(n_components=params['n_components']).fit(df.values)\n",
    "\n",
    "# GMM for input data\n",
    "gmm_input = GaussianMixture(n_components=params['n_components']).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence (all data): 1.6817099982342174\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "# Generate new samples\n",
    "samples_all = gmm_all.sample(params['n_samples'])[0]\n",
    "samples_input = gmm_all.sample(len(df.values))[0]\n",
    "\n",
    "def kl_div(p, q):\n",
    "    \"\"\"Compute the Kullback-Leibler divergence between two distributions.\"\"\"\n",
    "    return entropy(p, q)\n",
    "\n",
    "# Compute histograms\n",
    "hist_df_all, _ = np.histogram(df.values, bins=100, density=True)\n",
    "hist_samples_all, _ = np.histogram(samples_all, bins=100, density=True)\n",
    "\n",
    "# Compute KL divergence\n",
    "kl_div_all = kl_div(hist_df_all+1e-10, hist_samples_all+1e-10)\n",
    "\n",
    "print(f\"KL divergence (all data): {kl_div_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n",
      "21875/21875 [==============================] - 18s 820us/step\n"
     ]
    }
   ],
   "source": [
    "# Predict using generated samples\n",
    "predictions_all = model.predict(samples_all[:, len(x_columns):])\n",
    "predictions_input = model.predict(samples_input[:, len(x_columns):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000, 8)\n",
      "(700000, 8)\n",
      "(1000, 8)\n"
     ]
    }
   ],
   "source": [
    "print(Y_test.shape)\n",
    "print(predictions_input.shape)\n",
    "print(predictions_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [140000, 1000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_absolute_error\n\u001b[1;32m      3\u001b[0m \u001b[39m# Process and output best predictions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Note: you need to define how to determine the \"best\" output\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Assuming Y_test is available and corresponding to samples_all and samples_input\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[39m# Calculate MAE for all data samples\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m mae_all \u001b[39m=\u001b[39m mean_absolute_error(Y_test, predictions_all)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Calculate MAE for input data samples\u001b[39;00m\n\u001b[1;32m     11\u001b[0m mae_input \u001b[39m=\u001b[39m mean_absolute_error(Y_test, predictions_input)\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_regression.py:196\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_absolute_error\u001b[39m(\n\u001b[1;32m    142\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m ):\n\u001b[1;32m    144\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m    0.85...\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    197\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    200\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(np\u001b[39m.\u001b[39mabs(y_pred \u001b[39m-\u001b[39m y_true), weights\u001b[39m=\u001b[39msample_weight, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_regression.py:100\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     67\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m        correct keyword.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m    101\u001b[0m     y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    102\u001b[0m     y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [140000, 1000]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Process and output best predictions\n",
    "# Note: you need to define how to determine the \"best\" output\n",
    "# Assuming Y_test is available and corresponding to samples_all and samples_input\n",
    "\n",
    "# Calculate MAE for all data samples\n",
    "mae_all = mean_absolute_error(Y_test, predictions_all)\n",
    "\n",
    "# Calculate MAE for input data samples\n",
    "mae_input = mean_absolute_error(Y_test, predictions_input)\n",
    "\n",
    "print(f\"MAE for all data samples: {mae_all}\")\n",
    "print(f\"MAE for input data samples: {mae_input}\")\n",
    "\n",
    "# Determine which set of predictions has the lowest MAE\n",
    "best_predictions = predictions_all if mae_all < mae_input else predictions_input\n",
    "print(\"Best predictions:\", best_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def kl_div(p, q):\n",
    "    \"\"\"Compute the Kullback-Leibler divergence between two distributions.\"\"\"\n",
    "    return entropy(p, q)\n",
    "\n",
    "# Take a smaller subset of data for simplicity\n",
    "data_subset = np.random.choice(df.values.flatten(), 5000)\n",
    "samples_subset = np.random.choice(samples_all.flatten(), 5000)\n",
    "\n",
    "# Define bins\n",
    "bins = np.linspace(min(data_subset.min(), samples_subset.min()), \n",
    "                   max(data_subset.max(), samples_subset.max()), 100)\n",
    "\n",
    "# Method 1: Histogram\n",
    "hist_data, _ = np.histogram(data_subset, bins=bins, density=True)\n",
    "hist_samples, _ = np.histogram(samples_subset, bins=bins, density=True)\n",
    "\n",
    "# Method 2: Kernel Density Estimation (KDE)\n",
    "kde_data = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
    "kde_data.fit(data_subset[:, None])\n",
    "logprob_data = kde_data.score_samples(bins[:, None])\n",
    "\n",
    "kde_samples = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
    "kde_samples.fit(samples_subset[:, None])\n",
    "logprob_samples = kde_samples.score_samples(bins[:, None])\n",
    "\n",
    "# Method 3: Gaussian Mixture Models (GMM)\n",
    "gmm_data = GaussianMixture(n_components=2)\n",
    "gmm_data.fit(data_subset[:, None])\n",
    "logprob_gmm_data = gmm_data.score_samples(bins[:, None])\n",
    "\n",
    "gmm_samples = GaussianMixture(n_components=2)\n",
    "gmm_samples.fit(samples_subset[:, None])\n",
    "logprob_gmm_samples = gmm_samples.score_samples(bins[:, None])\n",
    "\n",
    "# KL divergence\n",
    "kl_div_hist = kl_div(hist_data+1e-10, hist_samples+1e-10)\n",
    "kl_div_kde = kl_div(np.exp(logprob_data), np.exp(logprob_samples))\n",
    "kl_div_gmm = kl_div(np.exp(logprob_gmm_data), np.exp(logprob_gmm_samples))\n",
    "\n",
    "# Print KL divergences\n",
    "print(f\"KL divergence (Histogram): {kl_div_hist}\")\n",
    "print(f\"KL divergence (KDE): {kl_div_kde}\")\n",
    "print(f\"KL divergence (GMM): {kl_div_gmm}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.plot(bins, hist_data, label='Data')\n",
    "plt.plot(bins, hist_samples, label='Samples')\n",
    "plt.title(f'Histogram (KL divergence = {kl_div_hist:.2f})')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(bins, np.exp(logprob_data), label='Data')\n",
    "plt.plot(bins, np.exp(logprob_samples), label='Samples')\n",
    "plt.title(f'KDE (KL divergence = {kl_div_kde:.2f})')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(bins, np.exp(logprob_gmm_data), label='Data')\n",
    "plt.plot(bins, np.exp(logprob_gmm_samples), label='Samples')\n",
    "plt.title(f'GMM (KL divergence = {kl_div_gmm:.2f})')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
